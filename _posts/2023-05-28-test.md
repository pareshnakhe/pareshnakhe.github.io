---
title: "The Eigen* Refresher"
categories:
- Online Learning
tags:
- content
- css
- edge case
- lists
- markup
---

**Disclaimer**: People have some basic understanding of linear alegbra.


This is the first of the 2 blog posts on topics of eigenvectors and how they relate to the technique of PCA. In this post, we'll revisit some basics of eigenvectors and their interpretation.

## Matrices as Linear Transformation Functions

To be able to interpret eigenvectors geometrically, we start with the interpretation of Matrices, specifically $n \times n$ matrices. Let's take the following example:

$$
A ~=~ \begin{bmatrix}
3 & 1\\
0 & 2
\end{bmatrix}
$$

<img class="wp-image-165 aligncenter" src="/assets/images/eigen_refresher/transformation_example.png" alt="poly" width="586" height="463" />

Multiplying this matrix with any point $\mathbf{x} = \begin{bmatrix}
x_1\\
x_2
\end{bmatrix}$ gives us the position of this point in the space defined by the transformation A. In this case, any such point is mapped to

$$
\begin{bmatrix}
3 & 1\\
0 & 2
\end{bmatrix}
\times
\begin{bmatrix}
x_1\\
x_2
\end{bmatrix} ~=~ \begin{bmatrix}
3x_1 + x_2\\
2x_2
\end{bmatrix}
$$

It's certainly not obvious but there are some special points (specifically vectors) that are not affected by this transformation. For example, $\begin{bmatrix}
1\\
0
\end{bmatrix}$. This vector maintains its direction but gets scaled during the process. This scaling factor is the eigenvalue associated with that vector.

$$
\begin{bmatrix}
3 & 1\\
0 & 2
\end{bmatrix}
\times
\begin{bmatrix}
1\\
0
\end{bmatrix} ~=~ 3\begin{bmatrix}
1\\
0
\end{bmatrix}
$$